Phase 3 Blog Post
Matthew Gittings

The third phase of the Data Science Course has been incredibly challenging. In this section we have gone over Linear Algebra, Calculus, and advanced Machine Learning.  Capping this off was the Phase Three Project which presented its own challenges. For this project I utilized a data set sourced through Kaggle for SyriaTel, a telecommunications company providing telephone service within Syria. The data set included information on 'Customer Churn' this would allow me to be able to create a model to predict the likelihood that a customer would stop using the services provided by SyriaTel. 
	After Label Encoding the data and performing Exploratory Data Analysis on the data set it seemed that Customer Service Calls and Account Length would be the most effectual way to predict churn. From here I went on the begin constructing my preliminary model. I first constructed a Pipeline to be able to more easily work with the data in modeling. I first began by using a K Nearest Neighbors method for classification. After cross-validation and grid searching I refined the model and ran it again. However with the KNN model there was a huge amount of data imbalance. This was being caused by the fact that there were many more instances of customers retention than customer churn within the data set. In order to combat this I saw fit to use the SMOTE method for combatting imbalance. Using this I was able to rectify the imbalance but it seemed that the KNN model was still overfitting the data. 
	Once it was clear that the KNN model was not the best option for modeling the data set I wet on the use the Random Forest Classifier. This proved to be a much better method for analyzing the data. With Random Forest I was able to fight both the imbalance, through SMOTE, as well as fight overfitting. 
	From this project I learned how hugely important the imbalance in the original data can be when trying to create a model. Without correcting the imbalance issue any model I would have made would have been flawed. The imbalance was causing any classifier to predict heavily toward a negative result-here meaning customer retention. As well it predicted very few positive results (customer churn) and in fact predicted more false negatives than positives. Correcting the imbalance allowed me to effectively work with the data and work to create a working model that was able to predict the data very well. 
